{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f96139",
   "metadata": {},
   "source": [
    "# 1. Introduction #\n",
    "\n",
    "### I will be web-scraping the information I need for analysing the used luxury sedan market in Singapore off sgcarmart.com, which is the largest online car marketplace in the country. ###\n",
    "\n",
    "To respect the the rules of sgcarmart.com, I visited its robot.txt these are the parameters I need to abide by:\n",
    "User-agent: *\n",
    "Crawl-delay: 5\n",
    "Disallow: /cgi-bin/\n",
    "Disallow: /images/\n",
    "Disallow: /mail/\n",
    "Disallow: /dealer/\n",
    "Disallow: /directory/premium/\n",
    "Disallow: /includes/\n",
    "Disallow: /phpads/\n",
    "Disallow: /update/\n",
    "Disallow: /upload/\n",
    "\n",
    "Hence, I will only be scraping these information: (1) Name of Postings (2) Price  (3) Depreciation Value (4) Mileage (5) Engine Capacity (6) Registered Date (7) Power (8) Number of Previous Owners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c926c5",
   "metadata": {},
   "source": [
    "# 2. Import Libraries #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a0e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from time import sleep\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932a13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sg_used_cars.csv','w',newline='') as f:\n",
    "    header = ['name','price','depre','mileage','engine_cap','reg_date','power','owners']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0579be1",
   "metadata": {},
   "source": [
    "# 3. Get Links For All Postings #\n",
    "\n",
    "I will store the links for all the car postings in a list before accessing them one by one to extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dea6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_links(soup):\n",
    "    links = []\n",
    "    for item in soup.findAll('strong'):\n",
    "        try:\n",
    "            link = item.find('a')\n",
    "            if 'info' in link['href']:\n",
    "                links.append(link['href'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return links\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7312073",
   "metadata": {},
   "source": [
    "# 4. Enter Link & Retrieve Info Needed #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0ed30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(soup):\n",
    "    try:\n",
    "        name = soup.find('div',{'id':'toMap'}).text.strip()\n",
    "    except:\n",
    "         name = 'NA'\n",
    "    return name\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find('td',{'class':'font_red'}).text.strip()[1:]\n",
    "    except:\n",
    "        price = 'NA'\n",
    "    return price\n",
    "\n",
    "def get_depre(soup):\n",
    "    try:\n",
    "        depre = re.findall(r'\\d+,\\d+',soup.findAll('tr',{'class':'row_bg'})[1].find('td',{'class':None}).text)[0]\n",
    "    except:\n",
    "        depre = 'NA'\n",
    "    return depre\n",
    "\n",
    "def get_miles(soup):\n",
    "    try:\n",
    "        mileage = re.findall(r'\\d+,\\d+',soup.find('div',{'class':'row_info'}).text.strip())[0]\n",
    "    except:\n",
    "        mileage = 'NA'\n",
    "    return mileage\n",
    "\n",
    "def get_engcap(soup):\n",
    "    try:\n",
    "        engine_cap = re.findall(r'\\d+,*\\d+',soup.findAll('div',{'class':'row_info'})[4].text)[0]\n",
    "    except:\n",
    "        engine_cap = 'NA'\n",
    "    return engine_cap\n",
    "\n",
    "def get_regdate(soup):\n",
    "    try:\n",
    "        reg_date = re.findall(r'\\d{2}-\\w{3}-\\d{4}',soup.findAll('tr',{'class':'row_bg'})[1].findAll('td',{'class':None})[-1].text)[0]\n",
    "    except:\n",
    "        reg_date = 'NA'\n",
    "    return reg_date\n",
    "\n",
    "def get_power(soup):\n",
    "    try:\n",
    "        power = re.findall(r'\\d+\\.\\d+',soup.findAll('div',{'class':'row_info'})[-2].text)[0]\n",
    "    except:\n",
    "        power = 'NA'\n",
    "    return power\n",
    "\n",
    "\n",
    "def get_owners(soup):\n",
    "    try:\n",
    "        owners = soup.findAll('div',{'class':'row_info'})[-1].text\n",
    "    except:\n",
    "        owners = 'NA'\n",
    "    return owners\n",
    "\n",
    "def access_link(links):\n",
    "    info = []\n",
    "    for link in links:\n",
    "        front = 'https://www.sgcarmart.com/used_cars/'\n",
    "        url = front + link\n",
    "        html = requests.get(url)\n",
    "        soup = bs(html.text,'lxml')\n",
    "        name = get_name(soup)\n",
    "        price = get_price(soup)\n",
    "        depre = get_depre(soup)\n",
    "        mileage = get_miles(soup)\n",
    "        eng_cap = get_engcap(soup)\n",
    "        reg_date = get_regdate(soup)\n",
    "        power = get_power(soup)\n",
    "        owners = get_owners(soup)\n",
    "        info.append([name,price,depre,mileage,eng_cap,reg_date,power,owners])\n",
    "        sleep(2)\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063408fb",
   "metadata": {},
   "source": [
    "# 5. Save to CSV file #\n",
    "\n",
    "After accessing all the links within the page, I will save them to my CSV file before scraping the following page. This is helpful for scraping large amount of information in the event that the script stop working midway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787d8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info(info):\n",
    "    \n",
    "    with open('sg_used_cars.csv','a',newline='') as f:\n",
    "\n",
    "        writer = csv.writer(f)\n",
    "        for row in info:\n",
    "            writer.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22234e73",
   "metadata": {},
   "source": [
    "# 6. Run Program #\n",
    "\n",
    "I put a 'print' counter to show me the number of pages that had been scraped and saved successfully each time so I know that the script is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e9ec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.sgcarmart.com/used_cars/listing.php?BRSR={}00&RPG=100&AVL=2&VEH=12'\n",
    "page_count = 0\n",
    "for page in range(10):\n",
    "    html = requests.get(url.format(page))\n",
    "    soup = bs(html.text,'lxml')\n",
    "    links = store_links(soup)\n",
    "    info = access_link(links)\n",
    "    save_info(info)\n",
    "    page_count += 1\n",
    "    print(page_count)\n",
    "    sleep(5)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
